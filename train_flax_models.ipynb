{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training KN flax models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urash/twouters/miniconda3/envs/nmma_gpu/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/urash/twouters/miniconda3/envs/nmma_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install afterglowpy if you want to simulate afterglows.\n",
      "Install wrapt_timeout_decorator if you want timeout simulations.\n",
      "[cuda(id=0)]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import inspect \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import arviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NMMA imports\n",
    "from nmma.em.training import SVDTrainingModel\n",
    "import nmma as nmma\n",
    "from nmma.em.io import read_photometry_files\n",
    "from nmma.em.utils import interpolate_nans\n",
    "import nmma.em.model_parameters as model_parameters\n",
    "\n",
    "### jax and friends\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn  # Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from flax import struct                # Flax dataclasses\n",
    "import optax\n",
    "\n",
    "print(jax.devices()) # check presence of CUDA is OK\n",
    "\n",
    "params = {\"axes.grid\": True,\n",
    "        \"text.usetex\" : True,\n",
    "        \"font.family\" : \"serif\",\n",
    "        \"ytick.color\" : \"black\",\n",
    "        \"xtick.color\" : \"black\",\n",
    "        \"axes.labelcolor\" : \"black\",\n",
    "        \"axes.edgecolor\" : \"black\",\n",
    "        \"font.serif\" : [\"Computer Modern Serif\"],\n",
    "        \"xtick.labelsize\": 16,\n",
    "        \"ytick.labelsize\": 16,\n",
    "        \"axes.labelsize\": 16,\n",
    "        \"legend.fontsize\": 16,\n",
    "        \"legend.title_fontsize\": 16,\n",
    "        \"figure.titlesize\": 16}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "# Get preprocessing function to read the data\n",
    "MODEL_FUNCTIONS = {\n",
    "    k: v for k, v in model_parameters.__dict__.items() if inspect.isfunction(v)\n",
    "}\n",
    "\n",
    "model_name = \"Bu2022Ye\"\n",
    "model_function = MODEL_FUNCTIONS[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if output directory exists and cleaning it...\n",
      "Cleaning data...\n",
      "Getting training data...\n",
      "Filters:\n",
      "['bessellux', 'bessellb', 'bessellv', 'bessellr', 'besselli', 'sdssu', 'ps1__g', 'ps1__r', 'ps1__i', 'ps1__z', 'ps1__y', 'uvot__b', 'uvot__u', 'uvot__uvm2', 'uvot__uvw1', 'uvot__uvw2', 'uvot__v', 'uvot__white', 'atlasc', 'atlaso', '2massj', '2massh', '2massks', 'ztfg', 'ztfr', 'ztfi']\n",
      "Getting the SVD model, start_training=False\n",
      "The grid will be interpolated to sample_time with interp1d\n",
      "Not loading new model\n"
     ]
    }
   ],
   "source": [
    "lcs_dir = \"/home/urash/twouters/KN_Lightcurves/lightcurves/lcs_bulla_2022\" # for remote SSH Potsdam\n",
    "filenames = os.listdir(lcs_dir)\n",
    "full_filenames = [os.path.join(lcs_dir, f) for f in filenames]\n",
    "\n",
    "out_dir = \"/home/urash/twouters/nmma_models/flax_models_new/\" # the trained models will be saved here\n",
    "# Check if out directory exists, if not, create it\n",
    "print(\"Checking if output directory exists and cleaning it...\")\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "# If the directory exists, clean it\n",
    "else:\n",
    "    for file in os.listdir(out_dir):\n",
    "        os.remove(os.path.join(out_dir, file))\n",
    "\n",
    "print(\"Cleaning data...\")\n",
    "data = read_photometry_files(full_filenames)\n",
    "data = interpolate_nans(data)\n",
    "\n",
    "print(\"Getting training data...\")\n",
    "training_data, parameters = model_function(data)\n",
    "\n",
    "key = list(training_data.keys())[0]\n",
    "example = training_data[key]\n",
    "t = example[\"t\"]\n",
    "keys = list(example.keys())\n",
    "filts = [k for k in keys if k not in parameters + [\"t\"]]\n",
    "print(\"Filters:\")\n",
    "print(filts)\n",
    "\n",
    "print(\"Getting the SVD model, start_training=False\")\n",
    "svd_ncoeff = 10\n",
    "training_model = SVDTrainingModel(\n",
    "        model_name,\n",
    "        training_data,\n",
    "        parameters,\n",
    "        t,\n",
    "        filts,\n",
    "        n_coeff=svd_ncoeff,\n",
    "        interpolation_type=\"flax\",\n",
    "        svd_path=out_dir,\n",
    "        start_training=False, # don't train, just prep the data, we train later on\n",
    "        load_model=False # don't load model, we train later on\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVD etc\n",
      "Normalizing mag filter bessellux...\n",
      "Normalizing mag filter bessellb...\n",
      "Normalizing mag filter bessellv...\n",
      "Normalizing mag filter bessellr...\n",
      "Normalizing mag filter besselli...\n",
      "Normalizing mag filter sdssu...\n",
      "Normalizing mag filter ps1__g...\n",
      "Normalizing mag filter ps1__r...\n",
      "Normalizing mag filter ps1__i...\n",
      "Normalizing mag filter ps1__z...\n",
      "Normalizing mag filter ps1__y...\n",
      "Normalizing mag filter uvot__b...\n",
      "Normalizing mag filter uvot__u...\n",
      "Normalizing mag filter uvot__uvm2...\n",
      "Normalizing mag filter uvot__uvw1...\n",
      "Normalizing mag filter uvot__uvw2...\n",
      "Normalizing mag filter uvot__v...\n",
      "Normalizing mag filter uvot__white...\n",
      "Normalizing mag filter atlasc...\n",
      "Normalizing mag filter atlaso...\n",
      "Normalizing mag filter 2massj...\n",
      "Normalizing mag filter 2massh...\n",
      "Normalizing mag filter 2massks...\n",
      "Normalizing mag filter ztfg...\n",
      "Normalizing mag filter ztfr...\n",
      "Normalizing mag filter ztfi...\n",
      "Fitting SVD etc DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting SVD etc\")\n",
    "svd_model = training_model.generate_svd_model()\n",
    "training_model.svd_model = svd_model\n",
    "print(\"Fitting SVD etc DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NN on SVD-decomposed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_model.train_model()\n",
    "\n",
    "# X = training_model.svd_model[filts[0]]['param_array_postprocess']\n",
    "# print(f\"Features have shape {X.shape}\")\n",
    "\n",
    "# y = training_model.svd_model[filts[0]]['cAmat'].T\n",
    "# _, output_ndim = y.shape\n",
    "# print(f\"Labels have shape {y.shape}\")\n",
    "\n",
    "# train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# training_model.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NN on pure lightcurve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['param_array_postprocess', 'param_mins', 'param_maxs', 'mins', 'maxs', 'data_postprocess', 'tt', 'n_coeff', 'cAmat', 'cAstd', 'VA'])\n"
     ]
    }
   ],
   "source": [
    "f = filts[0]\n",
    "example = training_model.svd_model[f]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[0.0826376  0.15969144 0.16007111 0.23861728 0.20389001 0.23419204\n",
      " 0.21398067 0.25880838 0.25490905 0.2163728  0.26100838 0.27132975\n",
      " 0.3287103  0.27528475 0.25608953 0.27083055 0.2250996  0.2271274\n",
      " 0.25033793 0.25257262 0.24311594 0.24934321 0.24304573 0.21190975\n",
      " 0.20025202 0.18777384 0.20451045 0.1688794  0.16265685 0.16674764\n",
      " 0.14438413 0.13087566 0.13134213 0.11371091 0.09845088 0.09084181\n",
      " 0.08185636 0.07280343 0.06862349 0.0710695  0.06948268 0.06184098\n",
      " 0.05235351 0.05395652 0.06808086 0.06734241 0.04973246 0.06139467\n",
      " 0.06115733 0.0680774  0.05455876 0.06639323 0.06043593 0.07564822\n",
      " 0.07554679 0.07642581 0.07406344 0.07451619 0.07369871 0.06734786\n",
      " 0.06828704 0.06667483 0.05962791 0.06161439 0.05520395 0.05569221\n",
      " 0.05585517 0.05421866 0.05890765 0.06243982 0.06608411 0.06731206\n",
      " 0.05943458 0.06467805 0.07009179 0.08482901 0.06662319 0.04768074\n",
      " 0.06146692 0.07485542 0.04930198 0.10634124 0.10679315 0.18439244\n",
      " 0.27207393 0.26129637 0.19121844 0.14393029 0.0612196  0.1097461\n",
      " 0.14557573 0.1680728  0.18540411 0.20884661 0.23625209 0.25688196\n",
      " 0.27339213 0.2868862  0.29810402 0.30757035]\n"
     ]
    }
   ],
   "source": [
    "example_lc = example[\"data_postprocess\"][0]\n",
    "print(np.shape(example_lc))\n",
    "print(example_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nmma.em.training.SVDTrainingModel at 0x7fd3d0207670>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing NN (using flax) for filter bessellux...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 18:02:27.461969: W external/tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.18MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16962016 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m state \u001b[39m=\u001b[39m utils_flax\u001b[39m.\u001b[39mcreate_train_state(net, jnp\u001b[39m.\u001b[39mones(input_ndim), init_key, config)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Perform training loop\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m state, train_losses, val_losses \u001b[39m=\u001b[39m utils_flax\u001b[39m.\u001b[39;49mtrain_loop(state, train_X, train_y, val_X, val_y, config)\n\u001b[1;32m     34\u001b[0m training_model\u001b[39m.\u001b[39msvd_model[filt][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m state\n",
      "File \u001b[0;32m~/nmma_gpu/nmma/em/utils_flax.py:226\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(state, train_X, train_y, val_X, val_y, config)\u001b[0m\n\u001b[1;32m    223\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    224\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnb_epochs):\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Do a single step\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     state, train_loss, val_loss \u001b[39m=\u001b[39m train_step(state, train_X, train_y, val_X, val_y)\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Save the losses\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nmma_gpu/lib/python3.10/site-packages/jax/_src/compiler.py:255\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    251\u001b[0m                          host_callbacks\u001b[39m=\u001b[39mhost_callbacks)\n\u001b[1;32m    252\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16962016 bytes."
     ]
    }
   ],
   "source": [
    "import nmma.em.utils_flax as utils_flax\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "for jj, filt in enumerate(training_model.filters):\n",
    "    # Split the random key to get a PRNG key for initialization of the network parameters\n",
    "    key, init_key = jax.random.split(key)\n",
    "    print(\"Computing NN (using flax) for filter %s...\" % filt)\n",
    "\n",
    "    param_array_postprocess = training_model.svd_model[filt][\"param_array_postprocess\"]\n",
    "    cAmat = training_model.svd_model[filt][\"cAmat\"]\n",
    "\n",
    "    train_X, val_X, train_y, val_y = train_test_split(\n",
    "        param_array_postprocess,\n",
    "        cAmat.T,\n",
    "        shuffle=True,\n",
    "        test_size=0.25,\n",
    "        random_state=training_model.random_seed,\n",
    "    )\n",
    "\n",
    "    # Config holds everything for the training setup\n",
    "    config = utils_flax.get_default_config()\n",
    "    # Input dimension can be found inside param array postprocess TODO can this be done more elegantly?\n",
    "    input_ndim = training_model.svd_model[filt][\"param_array_postprocess\"].shape[1]\n",
    "\n",
    "    # TODO - make architecture also part of config, if changed later on?\n",
    "    # Create neural network and initialize the state\n",
    "    net = utils_flax.MLP(layer_sizes=config.layer_sizes, act_func=config.act_func)\n",
    "    state = utils_flax.create_train_state(net, jnp.ones(input_ndim), init_key, config)\n",
    "\n",
    "    # Perform training loop\n",
    "    state, train_losses, val_losses = utils_flax.train_loop(state, train_X, train_y, val_X, val_y, config)\n",
    "    \n",
    "    training_model.svd_model[filt][\"model\"] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmma_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
